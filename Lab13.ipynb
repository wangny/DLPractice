{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\n",
      "L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!\n",
      "L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.\n",
      "\n",
      "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\n",
      "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L198', 'L199']\n",
      "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L200', 'L201', 'L202', 'L203']\n"
     ]
    }
   ],
   "source": [
    "# load lines dictionary \n",
    "lines = open('data/chatbot/movie_lines.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "\n",
    "# load conversations\n",
    "convs = open('data/chatbot/movie_conversations.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "\n",
    "print('\\n'.join(lines[:3]))\n",
    "print()\n",
    "print('\\n'.join(convs[:3]))\n",
    "\n",
    "w2idx = {}\n",
    "idx2w = {}\n",
    "w2idx['<UNK>'], idx2w[0] = 0, '<UNK>'\n",
    "w2idx['<BEG>'], idx2w[1] = 1, '<BEG>'\n",
    "w2idx['<END>'], idx2w[2] = 2, '<END>'\n",
    "\n",
    "INDEX = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "\n",
    "here I define some methods to 1.extract data from input files, 2.seperate encode and decode data, 3.prepare and tokenized data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lines(lines, line_num):\n",
    "    line_dict = [None] * line_num \n",
    "    for line in lines:\n",
    "        line = line.split(' +++$+++ ')\n",
    "        if len(line) < 3:\n",
    "            continue\n",
    "        idx = line[0].split('L')[1]\n",
    "        line_dict[int(idx)] = line[-1]\n",
    "    \n",
    "    return line_dict\n",
    "    \n",
    "def get_convs(convers):\n",
    "    result = []\n",
    "    for i in range(len(convers)):\n",
    "        conv = convers[i].split(\"'\")[1:-1]\n",
    "        conv = [ (int)(a.split('L')[1]) for a in conv if len(a)>3 ]\n",
    "        result.append(conv)\n",
    "        \n",
    "    return result\n",
    "\n",
    "def remove_unknown_words(lines):\n",
    "    dat = []\n",
    "    n = len(lines)\n",
    "    \n",
    "    for i in range(n):\n",
    "        if lines[i] is None or len(lines[i])<1:\n",
    "            dat.append(None)\n",
    "            continue\n",
    "            \n",
    "        s = lines[i]\n",
    "        s = ' '.join([w.lower() if w in nlp.vocab else '<UNK>' for w in s.split() ])\n",
    "        #s = ' '.join([w for w in s.split() if w in nlp.vocab])\n",
    "        s = '<BEG> '+s+' <END>'\n",
    "        \n",
    "        dat.append(s)\n",
    "    \n",
    "    return dat\n",
    "\n",
    "def cut_and_tokenize(lines, index, length=None,):\n",
    "    dat = []\n",
    "    n = len(lines)\n",
    "    str_length = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        if lines[i] is None or '<UNK>' in lines[i] or len(lines[i])<1:\n",
    "            dat.append(None)\n",
    "            continue\n",
    "            \n",
    "        s = lines[i].split()\n",
    "        str_length.append(len(s))\n",
    "        \n",
    "        for i in range(len(s)):\n",
    "            if s[i] in w2idx:\n",
    "                s[i] = w2idx[s[i]]\n",
    "            else:\n",
    "                w2idx[s[i]] = index\n",
    "                idx2w[index] = s[i]\n",
    "                s[i] = index\n",
    "                index += 1\n",
    "                \n",
    "        if length is not None:\n",
    "#            s = s + ['<PAD>']*(length-len(s))\n",
    "            s = s[:length]\n",
    "        \n",
    "        dat.append(s)\n",
    "    \n",
    "    print(\"max length : \", max(str_length))\n",
    "    print(\"avg length : \", sum(str_length)/len(str_length))\n",
    "    return dat, index\n",
    "\n",
    "def sep_enc_dec(conv_set, corpus):\n",
    "    enc, dec = [], []\n",
    "    for conv in conv_set:\n",
    "        for i in range(len(conv)-1):\n",
    "            if corpus[conv[i]] is None or corpus[conv[i+1]] is None:\n",
    "                continue\n",
    "            enc.append(conv[i])\n",
    "            dec.append(conv[++i])\n",
    "            \n",
    "    return enc, dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Details of preprocessing sentences\n",
    "1. I build a dictionary to tokenize data. \n",
    "2. To simplify training, I remove all sentences that contain words that are not listed in nlp.\n",
    "3. Add 'BEG' and 'END' tag to each sentence\n",
    "4. Remove any encode-decode pair if any of them contain 'UNK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length :  75\n",
      "avg length :  8.405328552883724\n"
     ]
    }
   ],
   "source": [
    "max_len = 75\n",
    "parsed_lines = get_lines(lines, 670000)\n",
    "parsed_lines = remove_unknown_words(parsed_lines)\n",
    "corpus, INDEX = cut_and_tokenize(parsed_lines, INDEX, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11462\n"
     ]
    }
   ],
   "source": [
    "conv_set = get_convs(convs)\n",
    "np.random.shuffle(conv_set)\n",
    "enc_set, dec_set = sep_enc_dec(conv_set, corpus)\n",
    "print(len(enc_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size :  15994\n"
     ]
    }
   ],
   "source": [
    "w2idx['<PAD>'] = INDEX\n",
    "idx2w[INDEX] = '<PAD>'\n",
    "INDEX += 1\n",
    "print(\"vocab size : \", INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del nlp\n",
    "del parsed_lines\n",
    "del lines\n",
    "del convs\n",
    "del conv_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch data\n",
    "pad every sentence that is shorter than max_length, batch encode and decode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchGenerator:\n",
    "    def __init__(self, enc, dec, corpus, max_len, batch_size):\n",
    "        assert len(enc) == len(dec)\n",
    "        \n",
    "        self.batch_num = len(enc)//batch_size\n",
    "        n = self.batch_num*batch_size\n",
    "        print(n)\n",
    "        \n",
    "        self.xs = [np.zeros(n, dtype=np.int16) for _ in range(max_len)] # encoder inputs\n",
    "        self.ys = [np.zeros(n, dtype=np.int16) for _ in range(max_len)] # decoder inputs\n",
    "        self.gs = [np.zeros(n, dtype=np.int16) for _ in range(max_len)] # decoder outputs\n",
    "        self.ws = [np.zeros(n, dtype=np.float16) for _ in range(max_len)] # decoder weight for loss caculation\n",
    "        \n",
    "        self.max_len = max_len\n",
    "        self.batch_size = batch_size\n",
    "        pad = w2idx['<PAD>']\n",
    "        \n",
    "        for b in range(self.batch_num):\n",
    "            for i in range(b*batch_size, (b+1)*batch_size):\n",
    "                enc_corpus = corpus[ enc[i] ]\n",
    "                dec_corpus = corpus[ dec[i] ]\n",
    "                for j in range(len(enc_corpus)-2):\n",
    "                    self.xs[j][i] = enc_corpus[j+1]\n",
    "                for j in range(j+1, max_len):\n",
    "                    self.xs[j][i] = pad\n",
    "                \n",
    "                for j in range(len(dec_corpus)-1):\n",
    "                    self.ys[j][i] = dec_corpus[j]\n",
    "                    self.gs[j][i] = dec_corpus[j+1]\n",
    "                    self.ws[j][i] = 1.0\n",
    "\n",
    "                for j in range(j+1, max_len): # don't forget padding and let loss weight zero\n",
    "                    self.ys[j][i] = pad\n",
    "                    self.gs[j][i] = pad\n",
    "                    self.ws[j][i] = 0.0\n",
    "    \n",
    "    def get(self, batch_id):\n",
    "        x = [self.xs[i][batch_id*self.batch_size:(batch_id+1)*self.batch_size] for i in range(self.max_len)]\n",
    "        y = [self.ys[i][batch_id*self.batch_size:(batch_id+1)*self.batch_size] for i in range(self.max_len)]\n",
    "        g = [self.gs[i][batch_id*self.batch_size:(batch_id+1)*self.batch_size] for i in range(self.max_len)]\n",
    "        w = [self.ws[i][batch_id*self.batch_size:(batch_id+1)*self.batch_size] for i in range(self.max_len)]\n",
    "        \n",
    "        return x, y, g, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11460\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "bath_generator = BatchGenerator(enc_set, dec_set, corpus, max_len, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del enc_set, dec_set, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq:\n",
    "    def __init__(self, enc_max_len, dec_max_len, vocab_size):\n",
    "        self.enc_max_len = enc_max_len\n",
    "        self.dec_max_len = dec_max_len\n",
    "        \n",
    "        with tf.variable_scope('seq2seq_intput/output'):\n",
    "            self.enc_inputs = [tf.placeholder(tf.int16, [None]) for i in range(enc_max_len)] # time mojor feed\n",
    "            self.dec_inputs = [tf.placeholder(tf.int16, [None]) for i in range(dec_max_len)]\n",
    "            self.groundtruths = [tf.placeholder(tf.int16, [None]) for i in range(enc_max_len)]\n",
    "            self.weights = [tf.placeholder(tf.float16, [None]) for i in range(dec_max_len)]\n",
    "            \n",
    "        with tf.variable_scope('seq2seq_rnn'): # training by teacher forcing\n",
    "            self.out_cell = tf.contrib.rnn.LSTMCell(256)\n",
    "            self.outputs, _ = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(self.enc_inputs, self.dec_inputs, \n",
    "                                                                                    self.out_cell, \n",
    "                                                                                    vocab_size, vocab_size, 200)\n",
    "        with tf.variable_scope('seq2seq_rnn', reuse=True): # predict by feeding previous\n",
    "            self.pred_cell = tf.contrib.rnn.LSTMCell(256, reuse=True) # reuse cell for train and test\n",
    "            self.predictions, _ = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(self.enc_inputs, self.dec_inputs, \n",
    "                                                                                        self.pred_cell, \n",
    "                                                                                        vocab_size, vocab_size, 200, \n",
    "                                                                                        feed_previous=True)\n",
    "        \n",
    "        with tf.variable_scope('loss'):\n",
    "            # caculate weighted loss\n",
    "            self.loss = tf.reduce_mean(tf.contrib.legacy_seq2seq.sequence_loss_by_example(self.outputs, \n",
    "                                                                                          self.groundtruths, \n",
    "                                                                                          self.weights))\n",
    "            self.optimizer = tf.train.AdamOptimizer(0.002).minimize(self.loss)\n",
    "        \n",
    "        config = tf.ConfigProto()\n",
    "        self.sess = tf.Session(config=config)\n",
    "        self.saver = tf.train.Saver()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def train(self, x, y, g, w):\n",
    "        fd = {}\n",
    "        for i in range(self.enc_max_len):\n",
    "            fd[self.enc_inputs[i]] = x[i] # show how to feed a list\n",
    "        \n",
    "        for i in range(self.dec_max_len):\n",
    "            fd[self.dec_inputs[i]] = y[i]\n",
    "            fd[self.groundtruths[i]] = g[i]\n",
    "            fd[self.weights[i]] = w[i]\n",
    "        \n",
    "        loss, _ = self.sess.run([self.loss, self.optimizer], fd)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def output(self, x, y):\n",
    "        fd = {}\n",
    "        for i in range(self.enc_max_len):\n",
    "            fd[self.enc_inputs[i]] = x[i]\n",
    "        \n",
    "        for i in range(self.dec_max_len):\n",
    "            fd[self.dec_inputs[i]] = y[i]\n",
    "        \n",
    "        out = self.sess.run(self.outputs, fd)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def predict(self, x, dec_beg):\n",
    "        fd = {}\n",
    "        for i in range(self.enc_max_len):\n",
    "            fd[self.enc_inputs[i]] = x[i]\n",
    "        \n",
    "        for i in range(self.dec_max_len): # when feed previous, the fist token should be '<BEG>', and others are useless\n",
    "            if i==0:\n",
    "                fd[self.dec_inputs[i]] = np.ones(y[i].shape, dtype=np.int32)*dec_beg\n",
    "            else:\n",
    "                fd[self.dec_inputs[i]] = np.zeros(y[i].shape, dtype=np.int32)\n",
    "        \n",
    "        pd = self.sess.run(self.predictions, fd)\n",
    "        \n",
    "        return pd\n",
    "    \n",
    "    def save(self, e):\n",
    "        self.saver.save(self.sess, 'model/seq2seq/seq2seq_%d.ckpt'%(e+1))\n",
    "    \n",
    "    def restore(self, e):\n",
    "        self.saver.restore(self.sess, 'model/seq2seq/seq2seq_%d.ckpt'%(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "model = Seq2Seq(max_len, max_len, INDEX)\n",
    "EPOCHS = 40\n",
    "batch_num = bath_generator.batch_num\n",
    "rec_loss = []\n",
    "for e in range(EPOCHS):\n",
    "    train_loss = 0\n",
    "    \n",
    "    for b in range(batch_num):\n",
    "        x, y, g, w = bath_generator.get(b)\n",
    "        batch_loss = model.train(x, y, g, w)\n",
    "        train_loss += batch_loss\n",
    "        print(b, end=\" \")\n",
    "    \n",
    "    train_loss /= batch_num\n",
    "    rec_loss.append(train_loss)\n",
    "    print(\"\\nepoch %d loss: %f\" % (e, train_loss))\n",
    "    \n",
    "    model.save(e)\n",
    "    \n",
    "np.save('./model/seq2seq/rec_loss.npy', rec_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I haven't successfully train a chatbot because my kernal always crush halfway through training. I guess there's something I did wrong that make my model taking up too much resource or become unstable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
